{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_tokenization_unicode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raqueeb/nlp_bangla/blob/master/NLP_tokenization_unicode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sq_5jXP5k4V",
        "colab_type": "text"
      },
      "source": [
        "## সহজ বাংলায় 'বাংলা' ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং (এনএলপি)\n",
        "\n",
        "### যন্ত্রের এবং মানুষের শেখার ধারণা একই\n",
        "\n",
        "ভালো করে লক্ষ্য করে দেখবেন যে একটা মেশিন এবং একটা বাচ্চার শেখার প্রসেস প্রায় একই। আমি আমাদের (আমার এবং স্বাতীর) বাচ্চাগুলোর উদাহরণ দেই। আমাদের বাচ্চাগুলো যখন কথা বলা শুরু করেছিল, তখন সে ভাষার কোন ধরনের গ্রামার সম্বন্ধে জানা তো দুরের কথা, তারা অক্ষরই জানতো না। তারা বিশেষ করে আমরা মানে বাবা-মা আত্মীয়-স্বজন সবার কাছ থেকে বিভিন্ন শব্দের স্যাম্পল শুনে শুনে কিছু একটা তৈরি করার চেষ্টা করত মাথার ভেতরে। তার মানে তারা শব্দগুলোকে ঠিকমত ডিকোড অর্থাৎ কোন শব্দগুলো কি, অথবা শব্দগুলো কোন অক্ষরগুলো দিয়ে তৈরি, কোনটার সাথে কোনটা লাগালে ভাব প্রকাশ হবে সেটা না বুঝেই বিভিন্ন শব্দ জোড়া লাগিয়ে কথা বলার চেষ্টা করতো। অনেক সময় সেই বাক্যগুলো মিনিংফুল হত, আবার অনেক সময় হতো না। এর অর্থ হচ্ছে একটা বাক্যে কিভাবে শব্দগুলোকে জোড়া লাগিয়ে ‘নাউন প্রনাউন ভার্ব অ্যাডযেক্টিভ’ না জেনে এমন কিছু তৈরি করত যা অনেক সময় মিলে যেত। যখন মিলত না তখন আমরা মানে মা-বাবা সেটাকে কারেকশন করে দিতাম। এরপর কয়েকদিন সে ভুলভাল বলে আস্তে আস্তে ঠিক করে নিতো তার শুরুর বাক্য তৈরি স্টাইল। আর সব কিছুই তারা করত কোন ধরনের  অক্ষর বা গ্রামার সম্বন্ধে ন্যূনতম ধারণা না নিয়েই। এটাই লক্ষ্য করার জিনিস। এবং ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এ সে কারণে আমরা অক্ষর নিয়ে খুব একটা মাথা ঘামাইনা। বরং শব্দগুলোর সিকোয়েন্স ম্যাটার করে বেশি। একই শব্দ আগে পড়ে বসলে মানে পাল্টে যায়। সেটা দেখবো সামনে।\n",
        "\n",
        "## অক্ষরজ্ঞান অথবা ব্যাকরণ শিখিয়ে এনএলপি হবে না\n",
        "\n",
        "এতক্ষণ যা বললাম তার একটা সারমর্ম করি এখানে। একটা বাচ্চা পৃথিবীতে এলো। তার আশেপাশের সবাই কথা বলছে বিভিন্ন ভঙ্গিতে। তাকে শেখানো হয়নি ভাষার রুল গুলোকে, বরং সে মাসের-পর-মাস দেখচ্ছে তার আশেপাশের মানুষজন কিভাবে যেন যোগাযোগ বলছে। উপায়ন্তর না দেখে কোন ধরনের রুলস না জেনে মানুষ কিভাবে কথা বলছে সেই প্যাটার্ন টা বের করে আশেপাশের মানুষ গুলোকে মিমিক করে কথা বলা শুরু করেছে বাচ্চাটা। বাচ্চাটার মাথার নিউরাল নেটওয়ার্ক আশেপাশের সব কিছু প্যাটার্ন ধরে ভাষার ব্যবহার শুরু করেছে কোন নিয়ম নীতি না জেনেই।\n",
        "\n",
        "মজার কথা হচ্ছে যখন আমি মেশিন লার্নিং এবং নিউরাল নেটওয়ার্ক নিয়ে কাজ করা শুরু করলাম, তখন বাচ্চাদের পুরো ব্যাপারটা মনে হতে থাকলো। বাচ্চাদের ছোটবেলার ব্যবহার আর নিউরাল নেটওয়ার্কের ব্যবহারের মধ্যে কোন পার্থক্য দেখি না। বড় হয়েও আমি ক্যাডেট কলেজে বাংলা গ্রামারে কোনরকমে পাস করে বের হতাম। আসলে ভাষা এমন একটা জিনিস যার জন্য কোন ধরনের কমপ্লিট ম্যানুয়াল তৈরি করা সম্ভব না, যেখানে আমরা বলতে পারি ভাষা এভাবেই হতে হবে, এর বাহিরে হলে চলবে না। ভাষায় এটা হলে বারোটা বেজে যেতো সাহিত্যের।  \n",
        "\n",
        "সেকারণে ছোটবেলায় আমাদের বাচ্চারা যেভাবে ভাষাকে তাদের কাজের সুবিধার জন্য ব্যবহার করছিল, সেটার অ্যাকুরেসি বাড়ছিল দিন দিন আশেপাশের ফিডব্যাক এর উপর ভিত্তি করে। তার মানে ওরা মাঝে মাঝে মুখস্থ করে, মাঝেমধ্যে গোঁজামিল দিয়ে, মাসের পর মাস এই একই ধরনের রিকার্সিভ প্রসেসে একটা মানসিক মডেল তৈরি করে ফেলতো তার মাথার ভেতরে, বিশেষ করে ভাষার ব্যবহার নিয়ে। বাচ্চাদের মাথা আশেপাশের প্যাটার্ন ধরে বুঝে ফেলতো কিভাবে একটা বাক্যে কোন শব্দের পরে কোন শব্দটা আসতে পারে - বিশেষ করে কোন শব্দটা না বললেই নয়। ফলে কোন ধরনের অক্ষর বা গ্রামার না শিখেও বাচ্চারা এক ধরনের জোড়াতালি দেওয়া বাক্য তৈরি করতে পারে - যা ইমপ্রুভ করে স্কুলে ভর্তি হবার আগেই। সবচেয়ে বড় কথা হচ্ছে সেটা কাজও করে বিশেষ করে আশেপাশের মানুষগুলোর কাছে।\n",
        "\n",
        "আর সেই কাজটাই আমি দেখছি গত কয়েক বছর ধরে ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এ। ডিপ লার্নিং মডেলকে কয়েকটা শেক্সপিয়ারের নাটক দিয়ে ট্রেনিং করিয়ে দিলে, সে শেক্সপিয়ারের মত করে কাছাকাছি নাটক লিখে ফেলতে পারে। আমরা কি সেই ডিপ লার্নিং মডেলকে গ্রামার বা অক্ষর শিখিয়েছিলাম? অবশ্যই না। ডিপ লার্নিং মডেল শেক্সপিয়ারের নাটকগুলো থেকে শব্দগুলোর একটা প্যাটার্ন মানে সিকোয়েন্স ধরে ধরে বের করে সেটাকেই ব্যবহার করে জোড়াতালি দিয়ে একটা নাটক লিখেছে - যা পড়লে বোঝা যায় অনেকখানি। আর এখন এতই এতই বাক্য কম্পিউটার দেখছে, সেখানে কম্পিউটারের জন্য একটা ভালো ন্যাচারাল ল্যাংগুয়েজ প্রসেসিং মডেল তৈরি করা খুব একটা কষ্টকর নয়। সেটা আরো ভালো বোঝা যায় যখন আমরা গুগল-সার্চে বসি। কোনরকমে একটা বাক্যের প্রথম শব্দ লিখলেই হলো, সে হাজারও অপশন দিবে তার পরবর্তী শব্দটা কি হতে পারে। এটাই ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিংয়ের একটা কাজ। আর মাথা খারাপ করবো না গল্প দিয়ে। চলুন হাতে-কলমে র অংশে।\n",
        "\n",
        "## টোকেনাইজেশন: বাক্যকে মিনিংফুল ছোট ছোট শব্দে ভাগ\n",
        "\n",
        "আচ্ছা বলুন তো কম্পিউটার কিভাবে আমাদের ভাষা প্রসেস করে? কম্পিউটার তো সংখ্যা ছাড়া কিছু বোঝেনা। সেখানে একটা বাক্যের ছোট ইউনিট হিসেবে শব্দ গুলোকে বলা যায় যা একেকটা অর্থ বহন করে। ভাষার একটা ছোট মিনিংফুল ইউনিট হচ্ছে শব্দগুলো। যেমন আমরা যদি একটা বাক্য না বলে শুধুমাত্র আম বলি, সেখানে একটা ধারণা পাওয়া যায় যে আমরা একটা ফলের নাম বলছি। আমরা সেই ফলটা খেতে চাচ্ছি। অথবা ফলটা আমার কাছে আছে। ভাব প্রকাশের জন্য একটা শব্দই অনেক সময় ভালো কাজ করে। আমরা শব্দগুলোকে এমনভাবে প্রসেস করব যাতে কম্পিউটার সেটাকে নিউরাল নেটওয়ার্ক পাঠাতে পারে। আমাদের নিউরাল নেটওয়ার্কের কাজ হবে শব্দগুলো থেকে তার ভাব গুলোকে বের করে নেওয়া। ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এর প্রথম পার্ট হচ্ছে ভাষার নরমালাইজেশন করা। এখানে আমাদের কাজ হবে বাক্যের মধ্যে শব্দগুলোর ঠিকমতো এনকোডিং করা। একে ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এর ভাষায় বলা হয় টোকেনাইজেশন। যার অর্থ হচ্ছে একটা বাক্যকে মিনিংফুল ছোট ছোট শব্দতে ভাগ করে সেটাকে সংখ্যায় এনকোড করা যা নিউরাল নেটওয়ার্ক বুঝতে পারে।\n",
        "\n",
        "শুরুতে একটা বাচ্চা তো অক্ষর জ্ঞান নিয়ে ভাষা শেখে না। সে শেখে শব্দগুলো দিয়ে, শব্দগুলোর প্লেসমেন্ট নিয়ে, কোন শব্দ আগে পড়ে গেলে শব্দের অর্থ গুলো কি রকম হতে পারে সেটা কে বের করার চেষ্টা করে। এই একই কাজ করবে নিউরাল নেটওয়ার্ক। এখন বলি কেন অক্ষর কাজ করবে না?কারণ আমরা ছোটবেলায় অক্ষর শিখে কথা বলা শুরু করিনি। কথা বলা শুরু করেছি আশেপাশে র মানুষ থেকে শব্দ শিখে।\n",
        "\n",
        "তবে ভাষার সবচেয়ে ছোট ইউনিট হচ্ছে একেকটা অক্ষর। কম্পিউটার সেই অক্ষর গুলো কিভাবে চেনে? অক্ষরগুলোকে একটা এনকোডিং স্কিমে ফেলে দেওয়া যায়, যার কাজ হচ্ছে অক্ষরকে সংখ্যায় পরিবর্তন করে নেওয়া। ইংরেজি শব্দের জন্য একটা পপুলার স্কিম হচ্ছে আ স্কি, বাংলা এনকোডিং এর জন্য ইউনিকোড খুব ভালো কাজ করে। কিন্তু অক্ষর দিয়ে একটা বাক্যের ভাব বোঝা যাবে?\n",
        "\n",
        "শুরুতে **এনএলপি** এর দুটো প্রিন্সিপাল।\n",
        "\n",
        "**১. টোকেনাইজেশন**\n",
        "\n",
        "**২. সিকোয়েন্সিং**\n",
        "\n",
        "## টোকেনাইজেশন, অক্ষরগুলোকে সংখ্যায় আনছি ইউনিকোডে"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNiyWex6qnnZ",
        "colab_type": "code",
        "outputId": "631ea1af-ef81-4dcd-9fb3-ec941cadbba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c='\\u0986'\n",
        "ord('অ')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2437"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXQvAzLcrlqf",
        "colab_type": "code",
        "outputId": "19ebf272-7834-4f90-e837-4da5d81ad1d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "chr(2437)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'অ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1dXe1Cet0Af",
        "colab_type": "code",
        "outputId": "ba926b67-2022-43d6-cec9-923013ed58ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c='\\u0986'\n",
        "ord('ক')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjhY7yu-E-L2",
        "colab_type": "text"
      },
      "source": [
        "ধরুন আমরা একটা শব্দ বলছি **কলস**, এর মধ্যে তিনটা অক্ষর আছে, যাকে আলাদা করে ইউনিকোড পয়েন্ট কোড হচ্ছে নিচের লাইনে।\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtSMH-0fuyJd",
        "colab_type": "code",
        "outputId": "81c80abe-08d5-48d6-e132-5da4acf742be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.array([ord(char) for char in u\"কলস\"])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2453, 2482, 2488])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICd9x0hKFQnB",
        "colab_type": "text"
      },
      "source": [
        "এই একই অক্ষর দিয়ে আরেকটা শব্দ হচ্ছে **সকল**, যার মধ্যে ইউনিকোড পয়েন্ট কোড একই আছে। আগের শব্দ এবং পরের শব্দের মধ্যে সংখ্যা একই, হয়তোবা একটু একটা সামনে অথবা আর একটা পেছনে। আর এই কারনেই অক্ষর দিয়ে একটা বাক্যের সেন্টিমেন্ট বা ভাব বোঝা সম্ভব না। সে কারণে আমরা বুঝতে পারছি অক্ষরকে এনকোড না করে শব্দকে এনকোড করলে আমাদের কাজে আউটপুট আসবে।\n",
        "\n",
        "এখানে, এনকোড ব্যাপারটাকে আমরা লেবেলিং বলতে পারি। প্রতিটা শব্দকে লেবেলিং করলে এর ভেতরে একটা প্যাটার্ন পাবো।\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJZBltPh26Z7",
        "colab_type": "code",
        "outputId": "f2839b10-e848-4bcd-8ed2-23835c8d3f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.strings.unicode_decode(\"সকল\", input_encoding='UTF-8')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([2488, 2453, 2482], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_PwbN51GBiL",
        "colab_type": "text"
      },
      "source": [
        "## কি বুঝলাম অক্ষর অথবা শব্দ দিয়ে এনকোড দিয়ে?\n",
        "\n",
        "**বাক্যের সেন্টিমেন্ট অথবা ভাবার্থ বের করার জন্য আমাদের অক্ষর এনকোড না করে শব্দকে লেবেল করতে হবে।**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwIIMDVNNd93",
        "colab_type": "text"
      },
      "source": [
        "ধরুন আমরা একটা বাক্য লিখছি।\n",
        "\n",
        "**আমি ভালোবাসি বই পড়তে।** \n",
        "\n",
        "এটাকে এনকোড করলাম অক্ষর দিয়ে। কোন প্যাটার্ন বোঝা যায়?\n",
        "\n",
        "না। এবং ব্যাপারটা কাজ করে না।\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lp0nAC843Ys",
        "colab_type": "code",
        "outputId": "eb1a8e4c-5982-47fd-ac12-8990c6f4f829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "tf.strings.unicode_decode('আমি ভালবাসি বই পড়তে', 'UTF-8')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(19,), dtype=int32, numpy=\n",
              "array([2438, 2478, 2495,   32, 2477, 2494, 2482, 2476, 2494, 2488, 2495,\n",
              "         32, 2476, 2439,   32, 2474, 2524, 2468, 2503], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWfdL2OCV7uJ",
        "colab_type": "text"
      },
      "source": [
        "বরং এখানে আমি শব্দকে আমরা সংখ্যা '১' দিলাম। এভাবে আমরা ভালোবাসি শব্দকে '২' এবং বই '৩' শেষে 'পড়তে' শব্দকে সংখ্যাকে '৪' দিলাম।\n",
        "\n",
        "**আমি ভালোবাসি বই পড়তে**\n",
        "\n",
        "--> ১ ------- ২ -------- ৩ -------- ৪\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmQQWBqGV8HL",
        "colab_type": "text"
      },
      "source": [
        "এখন কিছু কোড দেখব এই ব্যাপারটা হাতে-কলমে দেখার জন্য। আমরা যেহেতু **টেন্সরফ্লো** ব্যবহার করছি সে কারণে শুরু করব এর এপিআই দিয়ে। এই এপিআই কল করব পাইথন থেকে।\n",
        " \n",
        "মজার কথা হচ্ছে এই লেবেলিং করে দিচ্ছে Tokenizer নিজে থেকে। যখন আমরা Tokenizerকে ইনস্ট্যানশিয়েট করছি, সেখানে দুটো হাইপারপ্যারামিটার দিয়ে দিচ্ছি কাজের সুবিধার্থে। num_words ব্যবহার হচ্ছে সংখ্যায় - কতগুলো সবচেয়ে বেশি ব্যবহৃত শব্দগুলোকে টোকেনাইজ করতে হবে। শুরুতে ১০।\n",
        "\n",
        "যেহেতু বাংলায় দাড়ি, । একটা আলাদা ধরনের স্টপওয়ার্ড, সেকারণে ডিফল্ট স্টপওয়ার্ড ব্যবহার না করে দাড়ি এবং ! যোগ করে দিয়েছি। বাকি সব ডিফল্ট হিসেবে ছিলো এর ভেতর। বাংলা শব্দের জন্য দিয়েছি, ইংরেজি হলে দরকার ছিলো না।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zaCMcjMQifQc",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'আমি ভালবাসি বই পড়তে।'\n",
        "        ]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 10, filters='!।')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ljYh6GjHts",
        "colab_type": "text"
      },
      "source": [
        "ভালো করে লক্ষ্য করুন, fit_on_texts() মেথড আমাদের বাক্যকে এনকোড/লেবেলিং করে দিচ্ছে সংখ্যায়। \n",
        "\n",
        "আমাদের এখানে word_index মেথড ডিকশনারীর মতো জোড়া জোড়া করে কী-ভ্যালু তৈরি করে দেয় - যেখানে বাক্যের মধ্যে শব্দগুলো একটা করে কী, এর পাশে ভ্যালুটা হচ্ছে তার এসোসিয়েটেড লেবেল। \n",
        "\n",
        "এই ডিকশনারীটা প্রিন্ট করলেই ব্যাপারটা বোঝা যাবে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdNSi7gOm-3K",
        "colab_type": "code",
        "outputId": "656c26ac-0d85-4433-af2a-211beca5b51d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'আমি': 1, 'ভালবাসি': 2, 'বই': 3, 'পড়তে': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1jvtYj1mRvS",
        "colab_type": "code",
        "outputId": "ba616d51-2ce9-45c4-940c-0b95fb18e36d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.num_words"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvHMUVlEgT3D",
        "colab_type": "text"
      },
      "source": [
        "এভাবে আমরা আরেকটা শব্দ ধরতে পারি, ধরুন, **আমি ভালোবাসি বই লিখতে**। এখন দেখুন আমরা এর মধ্যে কমন সংখ্যা **আমি ভালোবাসি বই** শব্দগুলোকে সংখ্যা ১, ২ এবং ৩ দিয়ে দিয়েছে টোকেনাইজার। এখন আমাদের বাকি থাকছে **'লিখতে'** শব্দকে নতুন একটা এনকোডিং সংখ্যা দিতে। ধরা যাক আমরা সেটাকে দিয়ে দিলাম সংখ্যা ৫। দেখুন - আমাদের টোকেনাইজারও একই কাজ করেছে।\n",
        "\n",
        "এখন আমরা যখন দুটো শব্দের এনকোডিং/লেবেলিং দেখছি, ১২৩৪, এবং ১২৩৫, আমরা এখনই দুটি বাক্যের শব্দগুলোর মধ্যে বেশ কিছু সিমিলারিটি পাচ্ছি। কারণ এই সিমিলারিটি আমার **বইয়ের প্রতি ভালোবাসা** নিয়ে। শব্দকে একটা করে ইউনিক সংখ্যা অ্যাসাইন করে আমরা সংখ্যার মধ্যে সিমিলারিটি বুঝতে পারছি যা কম্পিউটারের জন্য বোঝা সোজা। এই শব্দকে এক একটা করে সংখ্যা দিয়ে এনকোড/লেবেলিং করাকে আমরা **টোকেনাইজেশন** বলছি। \n",
        "\n",
        "**আমি ভালোবাসি বই পড়তে**\n",
        "\n",
        "--> ১ ------- ২ -------- ৩ -------- ৪\n",
        "\n",
        "**আমি ভালোবাসি বই লিখতে**\n",
        "\n",
        "--> ১ ------- ২ -------- ৩ -------- ৫"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGSkb3Z8fcKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "    'আমি ভালবাসি বই পড়তে।',\n",
        "    'আমি ভালবাসি বই লিখতে!'\n",
        "        ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpcESKA6gC3H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21c4809c-2927-487b-e5e8-f6505c33ae78"
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'আমি': 1, 'ভালবাসি': 2, 'বই': 3, 'পড়তে': 4, 'লিখতে': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1-tCBXGu0wo",
        "colab_type": "text"
      },
      "source": [
        "word_index এখানে জোড়া জোড়া কী-ভ্যালু রিটার্ন করেছে। tokenizer সব ধরনের স্টপওয়ার্ড, মানে যতিচিহ্ন ফেলে দিয়েছে ডাটা ক্লিন রাখার জন্য।\n",
        "\n",
        "দেখে নেই এক শব্দ কতবার ব্যবহার হয়েছে word_counts দিয়ে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnReI1YflIyT",
        "colab_type": "code",
        "outputId": "8cf0b693-9abe-4afe-9026-c46dd922ef1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.word_counts"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('আমি', 3), ('ভালবাসি', 3), ('বই', 3), ('পড়তে', 2), ('লিখতে', 1)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7_gVyd_wqRw",
        "colab_type": "text"
      },
      "source": [
        "## tokenizer এর ভেতরের কনফিগারেশন\n",
        "\n",
        "মন ভালো হয়েছে তো এখন? tokenizer এর ভেতরে কিভাবে কাজ করছে সেটা না দেখলে আমার ঘুমই হয়না।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry6gfPTglrHM",
        "colab_type": "code",
        "outputId": "c13a60a9-67f5-4db3-9e92-4ece87df6d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "tokenizer.get_config()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'char_level': False,\n",
              " 'document_count': 3,\n",
              " 'filters': '!।',\n",
              " 'index_docs': '{\"2\": 3, \"4\": 2, \"1\": 3, \"3\": 3, \"5\": 1}',\n",
              " 'index_word': '{\"1\": \"\\\\u0986\\\\u09ae\\\\u09bf\", \"2\": \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\", \"3\": \"\\\\u09ac\\\\u0987\", \"4\": \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\", \"5\": \"\\\\u09b2\\\\u09bf\\\\u0996\\\\u09a4\\\\u09c7\"}',\n",
              " 'lower': True,\n",
              " 'num_words': 10,\n",
              " 'oov_token': None,\n",
              " 'split': ' ',\n",
              " 'word_counts': '{\"\\\\u0986\\\\u09ae\\\\u09bf\": 3, \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 3, \"\\\\u09ac\\\\u0987\": 3, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 2, \"\\\\u09b2\\\\u09bf\\\\u0996\\\\u09a4\\\\u09c7\": 1}',\n",
              " 'word_docs': '{\"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 3, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 2, \"\\\\u0986\\\\u09ae\\\\u09bf\": 3, \"\\\\u09ac\\\\u0987\": 3, \"\\\\u09b2\\\\u09bf\\\\u0996\\\\u09a4\\\\u09c7\": 1}',\n",
              " 'word_index': '{\"\\\\u0986\\\\u09ae\\\\u09bf\": 1, \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 2, \"\\\\u09ac\\\\u0987\": 3, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 4, \"\\\\u09b2\\\\u09bf\\\\u0996\\\\u09a4\\\\u09c7\": 5}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8MLY01HxL2n",
        "colab_type": "text"
      },
      "source": [
        "আরেকভাবে দেখি এখানে।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es0CguwQmkf0",
        "colab_type": "code",
        "outputId": "e0bbd4c7-da09-458a-9afe-0a7c897b98df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'আমি': 1, 'পড়তে': 4, 'বই': 3, 'ভালবাসি': 2, 'লিখতে': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-aTPGgtxonU",
        "colab_type": "text"
      },
      "source": [
        "এখন কর্পাস বানাবো কিভাবে? ও আচ্ছা, **কর্পাস** কি?"
      ]
    }
  ]
}
