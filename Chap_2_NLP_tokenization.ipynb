{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raqueeb/nlp_bangla/blob/master/Chap_2_NLP_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1s9CnVh9eO2"
      },
      "source": [
        "## টেক্সট প্রি-প্রসেসিং\n",
        "\n",
        "আমরা এর আগে হাতে-কলমে যেই কাজগুলো করলাম, সেটা ডাটা প্রি-প্রসেসিংয়ের অংশ। আমাদের বইগুলোতে বড় বাক্য থাকলে সেগুলোকে ভেঙে ছোট করে শব্দের টোকেন বানানোর যে প্রক্রিয়া সেটা এই ডাটা প্রি-প্রসেসিং এর অংশ। বাক্যের প্রতিটা শব্দকে ‘কর্পাসে’ ঢোকানোর জন্য শব্দের সিকোয়েন্স অর্থাৎ সেই শব্দটাকে প্রথমে টোকেন তৈরি করে সেটা থেকে সংখ্যার সিকোয়েন্সের জন্য যে কাজগুলো করতে হয় সেগুলো মডেল বিল্ডিং অথবা ডাটা এক্সপ্লোরেশন করার আগে ডাটা প্রি-প্রসেসিং অংশে করতে হবে। ডাটার ভেতরে যদি কোনো ‘নয়েজ’ থাকে, যেমন টেক্সট ফাইল এর জন্য হেডার, ফুটার অথবা ‘এইচটিএমএল’ বা ‘এক্সএমএল’, বিভিন্ন মার্কআপ, মেটাডাটা থাকলে সেগুলোকে ফেলে দেবো এই প্রি-প্রসেসিং এর অংশে। ফাইল যদি অন্য কোন ফরম্যাটে থাকে, যেমন ‘জেসন’ - সেই কনভার্শনগুলো প্রি-প্রসেসিং এর অংশ।\n", 
        "\n",
        "![alt text](https://raw.githubusercontent.com/raqueeb/nlp_bangla/master/assets/pre.png)\n",
        "\n",
        "ডাটার নরমালাইজেশন যেমন, ‘স্টেমিং’ এবং ‘লেমাটাইজেশন’, এগুলো বলে আপনাদের মাথা খারাপ করতে চাই না এই মুহূর্তে। মনে রাখতে হবে, টোকেনাইজেশনের পরে আমরা আর অক্ষরের পর্যায়ে থাকছিনা, সরাসরি চলে যাচ্ছি শব্দের পর্যায় - যেখানে এধরনের কিছু ‘নরমালাইজেশন’ করা যেতে পারে।\n",
        "এখানের উদাহরণগুলো একটু ভালো করে লক্ষ্য করুন। এই ব্যাপারগুলো নিয়ে আমরা সামনে আরো বিশদ করে আলাপ করব তবে এখানে ‘পয়েন্ট বাই পয়েন্ট’ ব্যাপারটা বোঝার চেষ্টা করুন।  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaCMcjMQifQc"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'আমি বই ভালবাসি',\n",
        "    'আমি বই পড়তে ভালবাসি!',\n",
        "    ]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1jvtYj1mRvS",
        "outputId": "2b7d134c-7b0e-4b8e-d1c6-a74d35c6cbd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.num_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI7YHfi7Bxtt"
      },
      "source": [
        "## ভোকাবুলারি,কর্পাস, ডিকশনারি\n",
        "\n",
        "এখানে fit_on_texts নিজের ভেতরের যে ভোকাবুলারি তৈরি থাকার কথা সেটাকে আপডেট করে নেয় এর মধ্যে যতগুলো শব্দ লিস্ট আকারে থাকে। আমরা যদি বলি “আমি যাই” সেটাকে একটা ডিকশনারি বানিয়ে দিবে এভাবে শব্দ দিয়ে। word_index[\"আমি\"] = 1; word_index[\"যাই\"] = 2, এভাবে চলতে থাকবে আরো যতো শব্দ থাকে। এটাকে আমরা শব্দের একটা ডিকশনারি বলতে পারি যেখানে প্রতিটা শব্দ একটা করে ইউনিক সংখ্যা (১,২,৩ …) পায়। এখানে শূন্যকে আলাদা করে রাখা হয়েছে প্যাডিংয়ের জন্য। তবে, সংখ্যার মান যত কম থাকবে ততবেশি সেই শব্দটা ‘কর্পাসে’ ব্যবহার হবে। এখানে ‘কর্পাস’ হচ্ছে শব্দের ডিকশনারি। তবে, আমাদের অভিজ্ঞতা বলে কিছু কিছু ‘স্টপ ওয়ার্ড’ বারবার ব্যবহার হয় বলে সেগুলোই আগে চলে আসতে পারে। সেজন্য আমরা অনেক সময় ‘স্টপ ওয়ার্ড’ ফেলে দিতে চাই।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdNSi7gOm-3K",
        "outputId": "cb2cb6bc-fdca-42c4-9b6b-f8e2cde521ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'আমি': 1, 'বই': 2, 'ভালবাসি': 3, 'পড়তে': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn9fr2RjB-KZ"
      },
      "source": [
        "আমাদের এই ডিকশনারিতে কি কি শব্দ আছে সেটাকে ক্রমানুসারে দেখি। তবে এখানে আমরা প্রতিটা শব্দ কয়বার করে আছে - সেটা দেখতে পারি।"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnReI1YflIyT",
        "outputId": "ea5c1775-e7b8-4e29-9a54-02ef42dbb94a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.word_counts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('আমি', 2), ('বই', 2), ('ভালবাসি', 2), ('পড়তে', 1)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk1qZjzEDUsO"
      },
      "source": [
        "## ‘টোকেনাইজারে’র বর্তমান ‘কনফিগারেশন’ কি আছে জানতে এই ফাংশন\n",
        "\n",
        "অনেক সময় ‘টোকেনাইজারে’র বর্তমান ‘কনফিগারেশন’ কি আছে - সেটা দেখার জন্য এই ফাংশনটা ব্যবহার করা যেতে পারে,  বিশেষ করে যখন কিছু কিছু ক্ষেত্রে টোকেনাইজার কেন এই কাজটা করল অথবা এই জিনিসটা কেন এলো - সেটা বোঝার জন্য এই ফাংশনটা খুবই জরুরী। "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry6gfPTglrHM",
        "outputId": "5ef13036-dc52-4783-e17d-54aa906ffdc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "tokenizer.get_config()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'char_level': False,\n",
              " 'document_count': 2,\n",
              " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
              " 'index_docs': '{\"2\": 2, \"1\": 2, \"3\": 2, \"4\": 1}',\n",
              " 'index_word': '{\"1\": \"\\\\u0986\\\\u09ae\\\\u09bf\", \"2\": \"\\\\u09ac\\\\u0987\", \"3\": \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\", \"4\": \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\"}',\n",
              " 'lower': True,\n",
              " 'num_words': 10,\n",
              " 'oov_token': None,\n",
              " 'split': ' ',\n",
              " 'word_counts': '{\"\\\\u0986\\\\u09ae\\\\u09bf\": 2, \"\\\\u09ac\\\\u0987\": 2, \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 2, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 1}',\n",
              " 'word_docs': '{\"\\\\u09ac\\\\u0987\": 2, \"\\\\u0986\\\\u09ae\\\\u09bf\": 2, \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 2, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 1}',\n",
              " 'word_index': '{\"\\\\u0986\\\\u09ae\\\\u09bf\": 1, \"\\\\u09ac\\\\u0987\": 2, \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 3, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 4}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}
